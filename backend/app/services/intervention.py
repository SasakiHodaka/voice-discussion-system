"""ä»‹å…¥ãƒ»åŠ©è¨€ç”Ÿæˆã‚µãƒ¼ãƒ“ã‚¹ - è­°è«–ã®çŠ¶æ…‹ã«å¿œã˜ãŸæ”¯æ´ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ç”Ÿæˆ."""

import logging
from typing import Dict, Any, List, Optional
from enum import Enum

from app.services.llm import LLMService

logger = logging.getLogger(__name__)


class InterventionType(str, Enum):
    """ä»‹å…¥ã‚¿ã‚¤ãƒ—."""

    CLARIFICATION = "clarification"  # æ˜ç¢ºåŒ–è¦æ±‚
    SUMMARY = "summary"  # è¦ç‚¹æ•´ç†
    PERSPECTIVE = "perspective"  # æ–°ã—ã„è¦–ç‚¹ã®æç¤º
    ENCOURAGEMENT = "encouragement"  # ç™ºè¨€ä¿ƒé€²
    CAUTION = "caution"  # æ³¨æ„å–šèµ·
    NONE = "none"  # ä»‹å…¥ä¸è¦


class InterventionService:
    """è­°è«–çŠ¶æ³ã«å¿œã˜ãŸä»‹å…¥ãƒ»åŠ©è¨€ã‚’ç”Ÿæˆã™ã‚‹ã‚µãƒ¼ãƒ“ã‚¹."""

    def __init__(self):
        """Initialize intervention service."""
        self.llm_service = LLMService()

        # ä»‹å…¥é–¾å€¤ã®è¨­å®š
        self.confusion_threshold = 0.6  # æ··ä¹±åº¦ãŒã“ã‚Œã‚’è¶…ãˆãŸã‚‰ä»‹å…¥
        self.stagnation_threshold = 0.7  # åœæ»åº¦ãŒã“ã‚Œã‚’è¶…ãˆãŸã‚‰ä»‹å…¥
        self.low_understanding_threshold = 0.4  # ç†è§£åº¦ãŒã“ã‚Œã‚’ä¸‹å›ã£ãŸã‚‰ä»‹å…¥

    def detect_intervention_need(
        self,
        segment_result: Dict[str, Any],
        participant_states: List[Dict[str, Any]],
    ) -> Dict[str, Any]:
        """
        ä»‹å…¥ãŒå¿…è¦ã‹ã‚’åˆ¤å®š.

        Args:
            segment_result: ã‚»ã‚°ãƒ¡ãƒ³ãƒˆåˆ†æçµæœ
            participant_states: å‚åŠ è€…ã®èªçŸ¥çŠ¶æ…‹ãƒªã‚¹ãƒˆ

        Returns:
            ä»‹å…¥å¿…è¦æ€§ã®åˆ¤å®šçµæœ
        """
        needs_intervention = False
        intervention_type = InterventionType.NONE
        priority = 0.0
        reason = ""

        # 1. æ··ä¹±åº¦ãƒã‚§ãƒƒã‚¯ï¼ˆMãƒ¡ãƒˆãƒªã‚¯ã‚¹ï¼‰
        confusion = segment_result.get("M", 0.0)
        if confusion > self.confusion_threshold:
            needs_intervention = True
            intervention_type = InterventionType.CLARIFICATION
            priority = max(priority, confusion)
            reason = f"é«˜ã„æ··ä¹±åº¦ãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸ (M={confusion:.2f})"

        # 2. åœæ»åº¦ãƒã‚§ãƒƒã‚¯ï¼ˆTãƒ¡ãƒˆãƒªã‚¯ã‚¹ï¼‰
        stagnation = segment_result.get("T", 0.0)
        if stagnation > self.stagnation_threshold:
            needs_intervention = True
            intervention_type = InterventionType.PERSPECTIVE
            priority = max(priority, stagnation)
            reason = f"è­°è«–ãŒåœæ»ã—ã¦ã„ã¾ã™ (T={stagnation:.2f})"

        # 3. å‚åŠ è€…ã®ç†è§£ä¸è¶³ãƒã‚§ãƒƒã‚¯
        low_understanding_count = 0
        for state in participant_states:
            if state.get("understanding_level", 1.0) < self.low_understanding_threshold:
                low_understanding_count += 1

        if low_understanding_count > 0:
            needs_intervention = True
            if intervention_type == InterventionType.NONE:
                intervention_type = InterventionType.SUMMARY
            priority = max(priority, 0.7)
            reason = f"{low_understanding_count}åã®å‚åŠ è€…ãŒç†è§£ã«å›°é›£ã‚’ç¤ºã—ã¦ã„ã¾ã™"

        # 4. è³ªå•ã¸ã®å›ç­”ä¸è¶³ãƒã‚§ãƒƒã‚¯
        Q_count = segment_result.get("Q", 0)
        A_count = segment_result.get("A", 0)
        if Q_count > 0 and A_count == 0:
            needs_intervention = True
            intervention_type = InterventionType.ENCOURAGEMENT
            priority = max(priority, 0.6)
            reason = f"{Q_count}ä»¶ã®è³ªå•ã«å›ç­”ãŒã‚ã‚Šã¾ã›ã‚“"

        # 5. ç™ºè¨€ã®æ¥µç«¯ãªåã‚Šãƒã‚§ãƒƒã‚¯
        utterances = segment_result.get("utterances", [])
        if len(utterances) > 0:
            speakers = [u.get("speaker") for u in utterances]
            dominant_speaker_ratio = speakers.count(max(set(speakers), key=speakers.count)) / len(
                speakers
            )
            if dominant_speaker_ratio > 0.7 and len(set(speakers)) > 1:
                needs_intervention = True
                intervention_type = InterventionType.ENCOURAGEMENT
                priority = max(priority, 0.5)
                reason = "ç™ºè¨€ãŒä¸€éƒ¨ã®å‚åŠ è€…ã«åã£ã¦ã„ã¾ã™"

        return {
            "needs_intervention": needs_intervention,
            "intervention_type": intervention_type.value,
            "priority": priority,
            "reason": reason,
        }

    def generate_intervention_message(
        self,
        intervention_type: str,
        context: Dict[str, Any],
    ) -> Optional[str]:
        """
        ä»‹å…¥ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ç”Ÿæˆ.

        Args:
            intervention_type: ä»‹å…¥ã‚¿ã‚¤ãƒ—
            context: ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±

        Returns:
            ç”Ÿæˆã•ã‚ŒãŸä»‹å…¥ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
        """
        try:
            itype = InterventionType(intervention_type)

            if itype == InterventionType.CLARIFICATION:
                return self._generate_clarification_message(context)
            elif itype == InterventionType.SUMMARY:
                return self._generate_summary_message(context)
            elif itype == InterventionType.PERSPECTIVE:
                return self._generate_perspective_message(context)
            elif itype == InterventionType.ENCOURAGEMENT:
                return self._generate_encouragement_message(context)
            elif itype == InterventionType.CAUTION:
                return self._generate_caution_message(context)
            else:
                return None

        except Exception as e:
            logger.error(f"Error generating intervention message: {e}")
            return None

    def _generate_clarification_message(self, context: Dict[str, Any]) -> str:
        """æ˜ç¢ºåŒ–ã‚’ä¿ƒã™ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸."""
        transcript = context.get("transcript", "")
        issues = context.get("issues", [])

        if self.llm_service.client and len(transcript) > 50:
            # LLMã‚’ä½¿ç”¨ã—ã¦è©³ç´°ãªåˆ†æ
            prompt = f"""
ä»¥ä¸‹ã®è­°è«–ã§æ··ä¹±ãŒç”Ÿã˜ã¦ã„ã¾ã™ã€‚ä½•ãŒä¸æ˜ç¢ºãªã®ã‹ã€ã©ã®ç‚¹ã‚’æ•´ç†ã™ã¹ãã‹ã‚’ææ¡ˆã—ã¦ãã ã•ã„ã€‚

è­°è«–å†…å®¹:
{transcript[:500]}

ææ¡ˆå½¢å¼:
- ã€Œã€œã®å®šç¾©ãŒæ›–æ˜§ã§ã™ã€‚å…·ä½“çš„ã«èª¬æ˜ã—ã¦ãã ã•ã„ã€
- ã€Œã€œã¨ã€œã®é–¢ä¿‚ãŒä¸æ˜ç¢ºã§ã™ã€‚æ•´ç†ã—ã¾ã—ã‚‡ã†ã€
"""
            try:
                response = self.llm_service.client.chat.completions.create(
                    model=self.llm_service.model,
                    messages=[{"role": "user", "content": prompt}],
                    temperature=0.7,
                    max_tokens=200,
                )
                return response.choices[0].message.content.strip()
            except Exception as e:
                logger.error(f"LLM call failed: {e}")

        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å®šå‹ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
        return "ğŸ’¡ è­°è«–ãŒè¤‡é›‘ã«ãªã£ã¦ã„ã¾ã™ã€‚ç¾åœ¨ã®è«–ç‚¹ã‚’æ•´ç†ã—ã€å…±é€šç†è§£ã‚’ç¢ºèªã—ã¾ã—ã‚‡ã†ã€‚"

    def _generate_summary_message(self, context: Dict[str, Any]) -> str:
        """è¦ç‚¹æ•´ç†ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸."""
        transcript = context.get("transcript", "")

        if self.llm_service.client and len(transcript) > 50:
            prompt = f"""
ä»¥ä¸‹ã®è­°è«–ã‚’ç°¡æ½”ã«è¦ç´„ã—ã€é‡è¦ãªãƒã‚¤ãƒ³ãƒˆã‚’3ç‚¹ä»¥å†…ã§ç®‡æ¡æ›¸ãã—ã¦ãã ã•ã„ã€‚

è­°è«–å†…å®¹:
{transcript[:500]}

å½¢å¼:
ğŸ“ ç¾åœ¨ã®è­°è«–ã®ãƒã‚¤ãƒ³ãƒˆ:
â€¢ [ãƒã‚¤ãƒ³ãƒˆ1]
â€¢ [ãƒã‚¤ãƒ³ãƒˆ2]
â€¢ [ãƒã‚¤ãƒ³ãƒˆ3]
"""
            try:
                response = self.llm_service.client.chat.completions.create(
                    model=self.llm_service.model,
                    messages=[{"role": "user", "content": prompt}],
                    temperature=0.5,
                    max_tokens=300,
                )
                return response.choices[0].message.content.strip()
            except Exception as e:
                logger.error(f"LLM call failed: {e}")

        return "ğŸ“ ã“ã“ã¾ã§ã®è­°è«–ã‚’æ•´ç†ã—ã¾ã—ã‚‡ã†ã€‚ä¸»è¦ãªãƒã‚¤ãƒ³ãƒˆã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚"

    def _generate_perspective_message(self, context: Dict[str, Any]) -> str:
        """æ–°ã—ã„è¦–ç‚¹æç¤ºãƒ¡ãƒƒã‚»ãƒ¼ã‚¸."""
        transcript = context.get("transcript", "")

        if self.llm_service.client and len(transcript) > 50:
            prompt = f"""
ä»¥ä¸‹ã®è­°è«–ãŒåœæ»ã—ã¦ã„ã¾ã™ã€‚æ–°ã—ã„è¦–ç‚¹ã‚„å•ã„ã‹ã‘ã‚’ææ¡ˆã—ã¦ãã ã•ã„ã€‚

è­°è«–å†…å®¹:
{transcript[:500]}

å½¢å¼:
ğŸ¤” ã“ã‚“ãªè¦–ç‚¹ã¯ã©ã†ã§ã—ã‚‡ã†ã‹ï¼Ÿ
[å…·ä½“çš„ãªå•ã„ã‹ã‘ã‚„è¦–ç‚¹]
"""
            try:
                response = self.llm_service.client.chat.completions.create(
                    model=self.llm_service.model,
                    messages=[{"role": "user", "content": prompt}],
                    temperature=0.8,
                    max_tokens=200,
                )
                return response.choices[0].message.content.strip()
            except Exception as e:
                logger.error(f"LLM call failed: {e}")

        return "ğŸ¤” è¦–ç‚¹ã‚’å¤‰ãˆã¦ã¿ã¾ã—ã‚‡ã†ã€‚åˆ¥ã®è§’åº¦ã‹ã‚‰è€ƒãˆã¦ã¿ã¾ã›ã‚“ã‹ï¼Ÿ"

    def _generate_encouragement_message(self, context: Dict[str, Any]) -> str:
        """ç™ºè¨€ä¿ƒé€²ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸."""
        Q_count = context.get("Q_count", 0)
        silent_participants = context.get("silent_participants", [])

        if Q_count > 0:
            return f"â“ {Q_count}ä»¶ã®è³ªå•ã«å›ç­”ãŒã‚ã‚Šã¾ã›ã‚“ã€‚èª°ã‹ç­”ãˆã‚‰ã‚Œã‚‹æ–¹ã¯ã„ã¾ã›ã‚“ã‹ï¼Ÿ"

        if silent_participants:
            names = ", ".join(silent_participants[:2])
            return f"ğŸ’¬ {names}ã•ã‚“ã®æ„è¦‹ã‚‚èã„ã¦ã¿ãŸã„ã§ã™ã€‚ã„ã‹ãŒã§ã—ã‚‡ã†ã‹ï¼Ÿ"

        return "ğŸ’¬ ä»–ã®æ–¹ã®æ„è¦‹ã‚‚èã„ã¦ã¿ã¾ã—ã‚‡ã†ã€‚"

    def _generate_caution_message(self, context: Dict[str, Any]) -> str:
        """æ³¨æ„å–šèµ·ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸."""
        issue = context.get("issue", "ä¸æ˜ãªå•é¡Œ")
        return f"âš ï¸ æ³¨æ„: {issue}ã€‚è­°è«–ã®é€²ã‚æ–¹ã‚’è¦‹ç›´ã—ã¾ã—ã‚‡ã†ã€‚"


# Singleton instance
intervention_service = InterventionService()
